{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Densely Connected Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt # generate an array fron a text file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# it will split the features and labels into a train set and a test set\n",
    "# This (train_...) also does randomized shuffling, so we don't have to worry about the labels being sorted by accident.\n",
    "# This will automatically shuffle them for us.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# we are passing the delimiter parameter to specify that the features are separated by a comma\n",
    "# data = genfromtxt('./data/labels_features.csv', delimiter=\",\")#, names=True, dtype=None)\n",
    "data = genfromtxt('./data/labels_features.csv', delimiter=\",\", names=True)#, dtype=None)\n",
    "NAMES = data.dtype.names\n",
    "data = genfromtxt('./data/labels_features.csv', delimiter=\",\")\n",
    "data = data[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could make it more universal I guess, but here it does not matter\n",
    "# label_index = len(data[1, :]) - 1\n",
    "# labels = data[:, label_index]\n",
    "# features_no = len(data[1, :]) - 1\n",
    "# features = data[:, 0:features_no]\n",
    "\n",
    "IDS = data[:, 0] # data[:, 0]\n",
    "LABELS = data[:, 1] # only class telling real / fake\n",
    "FEATURES = data[:, 2:] # only features, no class\n",
    "TARGET_NAMES = ['Metastasis, class 0', 'No metastasis, class 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.33\n",
    "\n",
    "dim = len(FEATURES[0])\n",
    "num_labels = len(np.unique(LABELS))\n",
    "\n",
    "# passing the X features; y labels; test size of 33%; random_state => seed to have the same shuffle every time\n",
    "# why 42? => https://news.mit.edu/2019/answer-life-universe-and-everything-sum-three-cubes-mathematics-0910\n",
    "x_train, x_test, y_train, y_test = train_test_split(FEATURES, LABELS, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force all the feature data to fall within a certain range\n",
    "# this can actually help the neural network perform better\n",
    "scaler_object = MinMaxScaler()\n",
    "\n",
    "# fit the scaler object to our training data\n",
    "# fit() finds the min and max value and then transform() is transforming the given array based on the MinMax we just calculated durring the fit\n",
    "scaler_object.fit(X_train)\n",
    "# we only fit to X_train and not X_test BECAUSE we do not want the scaler_object to peek at any test data - it would be cheating. If we would do that it is called data leakage and is essentially cheating. So we fit to the train data but transform both\n",
    "scaled_X_train = scaler_object.transform(X_train)\n",
    "scaled_X_test = scaler_object.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the model\n",
    "model = Sequential()\n",
    "\n",
    "# adding the layers\n",
    "# add the dense layer, expecting 4 features (we have 4 neurons), input dimention; activation function ReLu\n",
    "model.add(Dense(dim, input_dim = dim, activation = 'relu'))\n",
    "\n",
    "# here we can play arround with the neurons; too large / too small => bad results; we can do 1x or 2x input dimensions; we do not specify the input dim as it is not the input layer - it is a hidden layer\n",
    "model.add(Dense(8, activation= 'relu'))\n",
    "\n",
    "# 1 because we only have 1 neuron which has 1 output and is outputting the result of either 0 or 1; activation type sigmoid => fit between 0 and 1\n",
    "model.add(Dense(1, activation= 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      " - 0s - loss: 0.6783 - acc: 0.6667\n",
      "Epoch 2/400\n",
      " - 0s - loss: 0.6513 - acc: 0.7126\n",
      "Epoch 3/400\n",
      " - 0s - loss: 0.6309 - acc: 0.7126\n",
      "Epoch 4/400\n",
      " - 0s - loss: 0.6207 - acc: 0.7126\n",
      "Epoch 5/400\n",
      " - 0s - loss: 0.6108 - acc: 0.7126\n",
      "Epoch 6/400\n",
      " - 0s - loss: 0.6067 - acc: 0.7126\n",
      "Epoch 7/400\n",
      " - 0s - loss: 0.6032 - acc: 0.7126\n",
      "Epoch 8/400\n",
      " - 0s - loss: 0.6010 - acc: 0.7126\n",
      "Epoch 9/400\n",
      " - 0s - loss: 0.5982 - acc: 0.7126\n",
      "Epoch 10/400\n",
      " - 0s - loss: 0.5948 - acc: 0.7126\n",
      "Epoch 11/400\n",
      " - 0s - loss: 0.5908 - acc: 0.7126\n",
      "Epoch 12/400\n",
      " - 0s - loss: 0.5876 - acc: 0.7126\n",
      "Epoch 13/400\n",
      " - 0s - loss: 0.5850 - acc: 0.7126\n",
      "Epoch 14/400\n",
      " - 0s - loss: 0.5825 - acc: 0.7126\n",
      "Epoch 15/400\n",
      " - 0s - loss: 0.5788 - acc: 0.7126\n",
      "Epoch 16/400\n",
      " - 0s - loss: 0.5763 - acc: 0.7126\n",
      "Epoch 17/400\n",
      " - 0s - loss: 0.5725 - acc: 0.7126\n",
      "Epoch 18/400\n",
      " - 0s - loss: 0.5693 - acc: 0.7126\n",
      "Epoch 19/400\n",
      " - 0s - loss: 0.5662 - acc: 0.7126\n",
      "Epoch 20/400\n",
      " - 0s - loss: 0.5625 - acc: 0.7126\n",
      "Epoch 21/400\n",
      " - 0s - loss: 0.5594 - acc: 0.7126\n",
      "Epoch 22/400\n",
      " - 0s - loss: 0.5572 - acc: 0.7126\n",
      "Epoch 23/400\n",
      " - 0s - loss: 0.5533 - acc: 0.7126\n",
      "Epoch 24/400\n",
      " - 0s - loss: 0.5504 - acc: 0.7126\n",
      "Epoch 25/400\n",
      " - 0s - loss: 0.5473 - acc: 0.7126\n",
      "Epoch 26/400\n",
      " - 0s - loss: 0.5436 - acc: 0.7126\n",
      "Epoch 27/400\n",
      " - 0s - loss: 0.5399 - acc: 0.7126\n",
      "Epoch 28/400\n",
      " - 0s - loss: 0.5372 - acc: 0.7126\n",
      "Epoch 29/400\n",
      " - 0s - loss: 0.5332 - acc: 0.7126\n",
      "Epoch 30/400\n",
      " - 0s - loss: 0.5303 - acc: 0.7126\n",
      "Epoch 31/400\n",
      " - 0s - loss: 0.5263 - acc: 0.7126\n",
      "Epoch 32/400\n",
      " - 0s - loss: 0.5242 - acc: 0.7126\n",
      "Epoch 33/400\n",
      " - 0s - loss: 0.5190 - acc: 0.7126\n",
      "Epoch 34/400\n",
      " - 0s - loss: 0.5157 - acc: 0.7126\n",
      "Epoch 35/400\n",
      " - 0s - loss: 0.5110 - acc: 0.7241\n",
      "Epoch 36/400\n",
      " - 0s - loss: 0.5071 - acc: 0.7241\n",
      "Epoch 37/400\n",
      " - 0s - loss: 0.5054 - acc: 0.7356\n",
      "Epoch 38/400\n",
      " - 0s - loss: 0.5003 - acc: 0.7471\n",
      "Epoch 39/400\n",
      " - 0s - loss: 0.4961 - acc: 0.7586\n",
      "Epoch 40/400\n",
      " - 0s - loss: 0.4920 - acc: 0.7701\n",
      "Epoch 41/400\n",
      " - 0s - loss: 0.4883 - acc: 0.7701\n",
      "Epoch 42/400\n",
      " - 0s - loss: 0.4855 - acc: 0.7586\n",
      "Epoch 43/400\n",
      " - 0s - loss: 0.4801 - acc: 0.7701\n",
      "Epoch 44/400\n",
      " - 0s - loss: 0.4763 - acc: 0.7701\n",
      "Epoch 45/400\n",
      " - 0s - loss: 0.4724 - acc: 0.7586\n",
      "Epoch 46/400\n",
      " - 0s - loss: 0.4695 - acc: 0.7471\n",
      "Epoch 47/400\n",
      " - 0s - loss: 0.4658 - acc: 0.7471\n",
      "Epoch 48/400\n",
      " - 0s - loss: 0.4616 - acc: 0.7586\n",
      "Epoch 49/400\n",
      " - 0s - loss: 0.4577 - acc: 0.7586\n",
      "Epoch 50/400\n",
      " - 0s - loss: 0.4539 - acc: 0.7586\n",
      "Epoch 51/400\n",
      " - 0s - loss: 0.4510 - acc: 0.7471\n",
      "Epoch 52/400\n",
      " - 0s - loss: 0.4474 - acc: 0.7586\n",
      "Epoch 53/400\n",
      " - 0s - loss: 0.4436 - acc: 0.7586\n",
      "Epoch 54/400\n",
      " - 0s - loss: 0.4416 - acc: 0.7586\n",
      "Epoch 55/400\n",
      " - 0s - loss: 0.4382 - acc: 0.7586\n",
      "Epoch 56/400\n",
      " - 0s - loss: 0.4339 - acc: 0.7586\n",
      "Epoch 57/400\n",
      " - 0s - loss: 0.4308 - acc: 0.7701\n",
      "Epoch 58/400\n",
      " - 0s - loss: 0.4281 - acc: 0.7816\n",
      "Epoch 59/400\n",
      " - 0s - loss: 0.4257 - acc: 0.7701\n",
      "Epoch 60/400\n",
      " - 0s - loss: 0.4209 - acc: 0.7816\n",
      "Epoch 61/400\n",
      " - 0s - loss: 0.4178 - acc: 0.7816\n",
      "Epoch 62/400\n",
      " - 0s - loss: 0.4180 - acc: 0.7931\n",
      "Epoch 63/400\n",
      " - 0s - loss: 0.4115 - acc: 0.7931\n",
      "Epoch 64/400\n",
      " - 0s - loss: 0.4091 - acc: 0.8046\n",
      "Epoch 65/400\n",
      " - 0s - loss: 0.4069 - acc: 0.7816\n",
      "Epoch 66/400\n",
      " - 0s - loss: 0.4031 - acc: 0.8046\n",
      "Epoch 67/400\n",
      " - 0s - loss: 0.4005 - acc: 0.8046\n",
      "Epoch 68/400\n",
      " - 0s - loss: 0.3980 - acc: 0.7816\n",
      "Epoch 69/400\n",
      " - 0s - loss: 0.3941 - acc: 0.7931\n",
      "Epoch 70/400\n",
      " - 0s - loss: 0.3932 - acc: 0.8046\n",
      "Epoch 71/400\n",
      " - 0s - loss: 0.3909 - acc: 0.7931\n",
      "Epoch 72/400\n",
      " - 0s - loss: 0.3882 - acc: 0.7931\n",
      "Epoch 73/400\n",
      " - 0s - loss: 0.3830 - acc: 0.7931\n",
      "Epoch 74/400\n",
      " - 0s - loss: 0.3825 - acc: 0.8161\n",
      "Epoch 75/400\n",
      " - 0s - loss: 0.3791 - acc: 0.8046\n",
      "Epoch 76/400\n",
      " - 0s - loss: 0.3784 - acc: 0.7931\n",
      "Epoch 77/400\n",
      " - 0s - loss: 0.3748 - acc: 0.8046\n",
      "Epoch 78/400\n",
      " - 0s - loss: 0.3737 - acc: 0.8046\n",
      "Epoch 79/400\n",
      " - 0s - loss: 0.3695 - acc: 0.8046\n",
      "Epoch 80/400\n",
      " - 0s - loss: 0.3724 - acc: 0.7931\n",
      "Epoch 81/400\n",
      " - 0s - loss: 0.3632 - acc: 0.8046\n",
      "Epoch 82/400\n",
      " - 0s - loss: 0.3687 - acc: 0.8391\n",
      "Epoch 83/400\n",
      " - 0s - loss: 0.3652 - acc: 0.8391\n",
      "Epoch 84/400\n",
      " - 0s - loss: 0.3568 - acc: 0.8161\n",
      "Epoch 85/400\n",
      " - 0s - loss: 0.3569 - acc: 0.8161\n",
      "Epoch 86/400\n",
      " - 0s - loss: 0.3555 - acc: 0.8161\n",
      "Epoch 87/400\n",
      " - 0s - loss: 0.3516 - acc: 0.8506\n",
      "Epoch 88/400\n",
      " - 0s - loss: 0.3521 - acc: 0.8506\n",
      "Epoch 89/400\n",
      " - 0s - loss: 0.3476 - acc: 0.8391\n",
      "Epoch 90/400\n",
      " - 0s - loss: 0.3450 - acc: 0.8506\n",
      "Epoch 91/400\n",
      " - 0s - loss: 0.3451 - acc: 0.8276\n",
      "Epoch 92/400\n",
      " - 0s - loss: 0.3392 - acc: 0.8736\n",
      "Epoch 93/400\n",
      " - 0s - loss: 0.3391 - acc: 0.8851\n",
      "Epoch 94/400\n",
      " - 0s - loss: 0.3380 - acc: 0.8851\n",
      "Epoch 95/400\n",
      " - 0s - loss: 0.3362 - acc: 0.8276\n",
      "Epoch 96/400\n",
      " - 0s - loss: 0.3329 - acc: 0.8621\n",
      "Epoch 97/400\n",
      " - 0s - loss: 0.3331 - acc: 0.8736\n",
      "Epoch 98/400\n",
      " - 0s - loss: 0.3308 - acc: 0.8851\n",
      "Epoch 99/400\n",
      " - 0s - loss: 0.3345 - acc: 0.8276\n",
      "Epoch 100/400\n",
      " - 0s - loss: 0.3242 - acc: 0.8736\n",
      "Epoch 101/400\n",
      " - 0s - loss: 0.3305 - acc: 0.8506\n",
      "Epoch 102/400\n",
      " - 0s - loss: 0.3257 - acc: 0.8851\n",
      "Epoch 103/400\n",
      " - 0s - loss: 0.3217 - acc: 0.8736\n",
      "Epoch 104/400\n",
      " - 0s - loss: 0.3228 - acc: 0.8506\n",
      "Epoch 105/400\n",
      " - 0s - loss: 0.3189 - acc: 0.8621\n",
      "Epoch 106/400\n",
      " - 0s - loss: 0.3182 - acc: 0.8851\n",
      "Epoch 107/400\n",
      " - 0s - loss: 0.3152 - acc: 0.8966\n",
      "Epoch 108/400\n",
      " - 0s - loss: 0.3126 - acc: 0.8621\n",
      "Epoch 109/400\n",
      " - 0s - loss: 0.3086 - acc: 0.8736\n",
      "Epoch 110/400\n",
      " - 0s - loss: 0.3127 - acc: 0.8736\n",
      "Epoch 111/400\n",
      " - 0s - loss: 0.3069 - acc: 0.8851\n",
      "Epoch 112/400\n",
      " - 0s - loss: 0.3046 - acc: 0.8621\n",
      "Epoch 113/400\n",
      " - 0s - loss: 0.3059 - acc: 0.8966\n",
      "Epoch 114/400\n",
      " - 0s - loss: 0.2997 - acc: 0.8966\n",
      "Epoch 115/400\n",
      " - 0s - loss: 0.2974 - acc: 0.8966\n",
      "Epoch 116/400\n",
      " - 0s - loss: 0.2970 - acc: 0.8736\n",
      "Epoch 117/400\n",
      " - 0s - loss: 0.2952 - acc: 0.8736\n",
      "Epoch 118/400\n",
      " - 0s - loss: 0.2932 - acc: 0.8966\n",
      "Epoch 119/400\n",
      " - 0s - loss: 0.2906 - acc: 0.8851\n",
      "Epoch 120/400\n",
      " - 0s - loss: 0.2907 - acc: 0.8621\n",
      "Epoch 121/400\n",
      " - 0s - loss: 0.2878 - acc: 0.8966\n",
      "Epoch 122/400\n",
      " - 0s - loss: 0.2871 - acc: 0.8966\n",
      "Epoch 123/400\n",
      " - 0s - loss: 0.2847 - acc: 0.8851\n",
      "Epoch 124/400\n",
      " - 0s - loss: 0.2821 - acc: 0.8851\n",
      "Epoch 125/400\n",
      " - 0s - loss: 0.2815 - acc: 0.8966\n",
      "Epoch 126/400\n",
      " - 0s - loss: 0.2796 - acc: 0.8966\n",
      "Epoch 127/400\n",
      " - 0s - loss: 0.2822 - acc: 0.8851\n",
      "Epoch 128/400\n",
      " - 0s - loss: 0.2746 - acc: 0.8966\n",
      "Epoch 129/400\n",
      " - 0s - loss: 0.2745 - acc: 0.9080\n",
      "Epoch 130/400\n",
      " - 0s - loss: 0.2766 - acc: 0.8966\n",
      "Epoch 131/400\n",
      " - 0s - loss: 0.2711 - acc: 0.9080\n",
      "Epoch 132/400\n",
      " - 0s - loss: 0.2669 - acc: 0.9195\n",
      "Epoch 133/400\n",
      " - 0s - loss: 0.2692 - acc: 0.8966\n",
      "Epoch 134/400\n",
      " - 0s - loss: 0.2647 - acc: 0.9080\n",
      "Epoch 135/400\n",
      " - 0s - loss: 0.2647 - acc: 0.8736\n",
      "Epoch 136/400\n",
      " - 0s - loss: 0.2605 - acc: 0.8966\n",
      "Epoch 137/400\n",
      " - 0s - loss: 0.2618 - acc: 0.8966\n",
      "Epoch 138/400\n",
      " - 0s - loss: 0.2579 - acc: 0.9195\n",
      "Epoch 139/400\n",
      " - 0s - loss: 0.2560 - acc: 0.9195\n",
      "Epoch 140/400\n",
      " - 0s - loss: 0.2569 - acc: 0.8736\n",
      "Epoch 141/400\n",
      " - 0s - loss: 0.2508 - acc: 0.9310\n",
      "Epoch 142/400\n",
      " - 0s - loss: 0.2556 - acc: 0.9310\n",
      "Epoch 143/400\n",
      " - 0s - loss: 0.2498 - acc: 0.9195\n",
      "Epoch 144/400\n",
      " - 0s - loss: 0.2478 - acc: 0.8851\n",
      "Epoch 145/400\n",
      " - 0s - loss: 0.2497 - acc: 0.8736\n",
      "Epoch 146/400\n",
      " - 0s - loss: 0.2491 - acc: 0.9310\n",
      "Epoch 147/400\n",
      " - 0s - loss: 0.2478 - acc: 0.9310\n",
      "Epoch 148/400\n",
      " - 0s - loss: 0.2428 - acc: 0.9310\n",
      "Epoch 149/400\n",
      " - 0s - loss: 0.2386 - acc: 0.9310\n",
      "Epoch 150/400\n",
      " - 0s - loss: 0.2375 - acc: 0.9425\n",
      "Epoch 151/400\n",
      " - 0s - loss: 0.2382 - acc: 0.9310\n",
      "Epoch 152/400\n",
      " - 0s - loss: 0.2339 - acc: 0.9425\n",
      "Epoch 153/400\n",
      " - 0s - loss: 0.2346 - acc: 0.8966\n",
      "Epoch 154/400\n",
      " - 0s - loss: 0.2306 - acc: 0.9195\n",
      "Epoch 155/400\n",
      " - 0s - loss: 0.2293 - acc: 0.9540\n",
      "Epoch 156/400\n",
      " - 0s - loss: 0.2272 - acc: 0.9425\n",
      "Epoch 157/400\n",
      " - 0s - loss: 0.2252 - acc: 0.9540\n",
      "Epoch 158/400\n",
      " - 0s - loss: 0.2243 - acc: 0.9425\n",
      "Epoch 159/400\n",
      " - 0s - loss: 0.2193 - acc: 0.9540\n",
      "Epoch 160/400\n",
      " - 0s - loss: 0.2212 - acc: 0.9540\n",
      "Epoch 161/400\n",
      " - 0s - loss: 0.2175 - acc: 0.9540\n",
      "Epoch 162/400\n",
      " - 0s - loss: 0.2166 - acc: 0.9540\n",
      "Epoch 163/400\n",
      " - 0s - loss: 0.2153 - acc: 0.9655\n",
      "Epoch 164/400\n",
      " - 0s - loss: 0.2178 - acc: 0.9540\n",
      "Epoch 165/400\n",
      " - 0s - loss: 0.2135 - acc: 0.9540\n",
      "Epoch 166/400\n",
      " - 0s - loss: 0.2137 - acc: 0.9310\n",
      "Epoch 167/400\n",
      " - 0s - loss: 0.2082 - acc: 0.9655\n",
      "Epoch 168/400\n",
      " - 0s - loss: 0.2103 - acc: 0.9655\n",
      "Epoch 169/400\n",
      " - 0s - loss: 0.2057 - acc: 0.9655\n",
      "Epoch 170/400\n",
      " - 0s - loss: 0.2031 - acc: 0.9655\n",
      "Epoch 171/400\n",
      " - 0s - loss: 0.2015 - acc: 0.9655\n",
      "Epoch 172/400\n",
      " - 0s - loss: 0.2009 - acc: 0.9655\n",
      "Epoch 173/400\n",
      " - 0s - loss: 0.1995 - acc: 0.9655\n",
      "Epoch 174/400\n",
      " - 0s - loss: 0.1980 - acc: 0.9655\n",
      "Epoch 175/400\n",
      " - 0s - loss: 0.1966 - acc: 0.9655\n",
      "Epoch 176/400\n",
      " - 0s - loss: 0.1946 - acc: 0.9655\n",
      "Epoch 177/400\n",
      " - 0s - loss: 0.1948 - acc: 0.9655\n",
      "Epoch 178/400\n",
      " - 0s - loss: 0.1920 - acc: 0.9655\n",
      "Epoch 179/400\n",
      " - 0s - loss: 0.1899 - acc: 0.9655\n",
      "Epoch 180/400\n",
      " - 0s - loss: 0.1885 - acc: 0.9655\n",
      "Epoch 181/400\n",
      " - 0s - loss: 0.1876 - acc: 0.9655\n",
      "Epoch 182/400\n",
      " - 0s - loss: 0.1857 - acc: 0.9655\n",
      "Epoch 183/400\n",
      " - 0s - loss: 0.1844 - acc: 0.9655\n",
      "Epoch 184/400\n",
      " - 0s - loss: 0.1874 - acc: 0.9655\n",
      "Epoch 185/400\n",
      " - 0s - loss: 0.1887 - acc: 0.9655\n",
      "Epoch 186/400\n",
      " - 0s - loss: 0.1849 - acc: 0.9655\n",
      "Epoch 187/400\n",
      " - 0s - loss: 0.1771 - acc: 0.9655\n",
      "Epoch 188/400\n",
      " - 0s - loss: 0.1838 - acc: 0.9655\n",
      "Epoch 189/400\n",
      " - 0s - loss: 0.1769 - acc: 0.9655\n",
      "Epoch 190/400\n",
      " - 0s - loss: 0.1770 - acc: 0.9655\n",
      "Epoch 191/400\n",
      " - 0s - loss: 0.1769 - acc: 0.9655\n",
      "Epoch 192/400\n",
      " - 0s - loss: 0.1730 - acc: 0.9655\n",
      "Epoch 193/400\n",
      " - 0s - loss: 0.1738 - acc: 0.9655\n",
      "Epoch 194/400\n",
      " - 0s - loss: 0.1716 - acc: 0.9655\n",
      "Epoch 195/400\n",
      " - 0s - loss: 0.1707 - acc: 0.9655\n",
      "Epoch 196/400\n",
      " - 0s - loss: 0.1687 - acc: 0.9655\n",
      "Epoch 197/400\n",
      " - 0s - loss: 0.1678 - acc: 0.9655\n",
      "Epoch 198/400\n",
      " - 0s - loss: 0.1661 - acc: 0.9655\n",
      "Epoch 199/400\n",
      " - 0s - loss: 0.1668 - acc: 0.9655\n",
      "Epoch 200/400\n",
      " - 0s - loss: 0.1636 - acc: 0.9655\n",
      "Epoch 201/400\n",
      " - 0s - loss: 0.1651 - acc: 0.9655\n",
      "Epoch 202/400\n",
      " - 0s - loss: 0.1596 - acc: 0.9655\n",
      "Epoch 203/400\n",
      " - 0s - loss: 0.1638 - acc: 0.9655\n",
      "Epoch 204/400\n",
      " - 0s - loss: 0.1588 - acc: 0.9655\n",
      "Epoch 205/400\n",
      " - 0s - loss: 0.1586 - acc: 0.9655\n",
      "Epoch 206/400\n",
      " - 0s - loss: 0.1560 - acc: 0.9655\n",
      "Epoch 207/400\n",
      " - 0s - loss: 0.1561 - acc: 0.9655\n",
      "Epoch 208/400\n",
      " - 0s - loss: 0.1537 - acc: 0.9655\n",
      "Epoch 209/400\n",
      " - 0s - loss: 0.1532 - acc: 0.9655\n",
      "Epoch 210/400\n",
      " - 0s - loss: 0.1525 - acc: 0.9655\n",
      "Epoch 211/400\n",
      " - 0s - loss: 0.1519 - acc: 0.9655\n",
      "Epoch 212/400\n",
      " - 0s - loss: 0.1504 - acc: 0.9655\n",
      "Epoch 213/400\n",
      " - 0s - loss: 0.1501 - acc: 0.9655\n",
      "Epoch 214/400\n",
      " - 0s - loss: 0.1471 - acc: 0.9655\n",
      "Epoch 215/400\n",
      " - 0s - loss: 0.1497 - acc: 0.9655\n",
      "Epoch 216/400\n",
      " - 0s - loss: 0.1458 - acc: 0.9655\n",
      "Epoch 217/400\n",
      " - 0s - loss: 0.1452 - acc: 0.9655\n",
      "Epoch 218/400\n",
      " - 0s - loss: 0.1456 - acc: 0.9655\n",
      "Epoch 219/400\n",
      " - 0s - loss: 0.1423 - acc: 0.9655\n",
      "Epoch 220/400\n",
      " - 0s - loss: 0.1421 - acc: 0.9655\n",
      "Epoch 221/400\n",
      " - 0s - loss: 0.1421 - acc: 0.9655\n",
      "Epoch 222/400\n",
      " - 0s - loss: 0.1398 - acc: 0.9655\n",
      "Epoch 223/400\n",
      " - 0s - loss: 0.1395 - acc: 0.9655\n",
      "Epoch 224/400\n",
      " - 0s - loss: 0.1382 - acc: 0.9655\n",
      "Epoch 225/400\n",
      " - 0s - loss: 0.1367 - acc: 0.9655\n",
      "Epoch 226/400\n",
      " - 0s - loss: 0.1370 - acc: 0.9655\n",
      "Epoch 227/400\n",
      " - 0s - loss: 0.1351 - acc: 0.9655\n",
      "Epoch 228/400\n",
      " - 0s - loss: 0.1343 - acc: 0.9655\n",
      "Epoch 229/400\n",
      " - 0s - loss: 0.1333 - acc: 0.9655\n",
      "Epoch 230/400\n",
      " - 0s - loss: 0.1333 - acc: 0.9655\n",
      "Epoch 231/400\n",
      " - 0s - loss: 0.1314 - acc: 0.9655\n",
      "Epoch 232/400\n",
      " - 0s - loss: 0.1308 - acc: 0.9655\n",
      "Epoch 233/400\n",
      " - 0s - loss: 0.1310 - acc: 0.9655\n",
      "Epoch 234/400\n",
      " - 0s - loss: 0.1296 - acc: 0.9655\n",
      "Epoch 235/400\n",
      " - 0s - loss: 0.1281 - acc: 0.9655\n",
      "Epoch 236/400\n",
      " - 0s - loss: 0.1277 - acc: 0.9655\n",
      "Epoch 237/400\n",
      " - 0s - loss: 0.1266 - acc: 0.9655\n",
      "Epoch 238/400\n",
      " - 0s - loss: 0.1261 - acc: 0.9655\n",
      "Epoch 239/400\n",
      " - 0s - loss: 0.1257 - acc: 0.9885\n",
      "Epoch 240/400\n",
      " - 0s - loss: 0.1258 - acc: 0.9885\n",
      "Epoch 241/400\n",
      " - 0s - loss: 0.1249 - acc: 0.9770\n",
      "Epoch 242/400\n",
      " - 0s - loss: 0.1221 - acc: 0.9770\n",
      "Epoch 243/400\n",
      " - 0s - loss: 0.1237 - acc: 0.9885\n",
      "Epoch 244/400\n",
      " - 0s - loss: 0.1237 - acc: 0.9885\n",
      "Epoch 245/400\n",
      " - 0s - loss: 0.1212 - acc: 0.9770\n",
      "Epoch 246/400\n",
      " - 0s - loss: 0.1203 - acc: 0.9885\n",
      "Epoch 247/400\n",
      " - 0s - loss: 0.1193 - acc: 0.9885\n",
      "Epoch 248/400\n",
      " - 0s - loss: 0.1189 - acc: 0.9770\n",
      "Epoch 249/400\n",
      " - 0s - loss: 0.1177 - acc: 0.9770\n",
      "Epoch 250/400\n",
      " - 0s - loss: 0.1169 - acc: 0.9770\n",
      "Epoch 251/400\n",
      " - 0s - loss: 0.1170 - acc: 0.9885\n",
      "Epoch 252/400\n",
      " - 0s - loss: 0.1163 - acc: 0.9885\n",
      "Epoch 253/400\n",
      " - 0s - loss: 0.1157 - acc: 0.9770\n",
      "Epoch 254/400\n",
      " - 0s - loss: 0.1154 - acc: 0.9885\n",
      "Epoch 255/400\n",
      " - 0s - loss: 0.1168 - acc: 0.9885\n",
      "Epoch 256/400\n",
      " - 0s - loss: 0.1145 - acc: 0.9885\n",
      "Epoch 257/400\n",
      " - 0s - loss: 0.1134 - acc: 0.9885\n",
      "Epoch 258/400\n",
      " - 0s - loss: 0.1123 - acc: 0.9885\n",
      "Epoch 259/400\n",
      " - 0s - loss: 0.1118 - acc: 0.9885\n",
      "Epoch 260/400\n",
      " - 0s - loss: 0.1109 - acc: 0.9885\n",
      "Epoch 261/400\n",
      " - 0s - loss: 0.1112 - acc: 0.9770\n",
      "Epoch 262/400\n",
      " - 0s - loss: 0.1097 - acc: 0.9885\n",
      "Epoch 263/400\n",
      " - 0s - loss: 0.1110 - acc: 0.9885\n",
      "Epoch 264/400\n",
      " - 0s - loss: 0.1083 - acc: 0.9885\n",
      "Epoch 265/400\n",
      " - 0s - loss: 0.1101 - acc: 0.9885\n",
      "Epoch 266/400\n",
      " - 0s - loss: 0.1076 - acc: 0.9885\n",
      "Epoch 267/400\n",
      " - 0s - loss: 0.1082 - acc: 0.9885\n",
      "Epoch 268/400\n",
      " - 0s - loss: 0.1060 - acc: 0.9885\n",
      "Epoch 269/400\n",
      " - 0s - loss: 0.1052 - acc: 0.9885\n",
      "Epoch 270/400\n",
      " - 0s - loss: 0.1042 - acc: 0.9885\n",
      "Epoch 271/400\n",
      " - 0s - loss: 0.1041 - acc: 0.9885\n",
      "Epoch 272/400\n",
      " - 0s - loss: 0.1031 - acc: 0.9885\n",
      "Epoch 273/400\n",
      " - 0s - loss: 0.1024 - acc: 0.9885\n",
      "Epoch 274/400\n",
      " - 0s - loss: 0.1022 - acc: 0.9885\n",
      "Epoch 275/400\n",
      " - 0s - loss: 0.1038 - acc: 0.9885\n",
      "Epoch 276/400\n",
      " - 0s - loss: 0.1040 - acc: 0.9885\n",
      "Epoch 277/400\n",
      " - 0s - loss: 0.1065 - acc: 0.9885\n",
      "Epoch 278/400\n",
      " - 0s - loss: 0.1020 - acc: 0.9885\n",
      "Epoch 279/400\n",
      " - 0s - loss: 0.1023 - acc: 0.9885\n",
      "Epoch 280/400\n",
      " - 0s - loss: 0.0996 - acc: 0.9885\n",
      "Epoch 281/400\n",
      " - 0s - loss: 0.0998 - acc: 0.9885\n",
      "Epoch 282/400\n",
      " - 0s - loss: 0.0985 - acc: 0.9885\n",
      "Epoch 283/400\n",
      " - 0s - loss: 0.0978 - acc: 0.9885\n",
      "Epoch 284/400\n",
      " - 0s - loss: 0.0975 - acc: 0.9885\n",
      "Epoch 285/400\n",
      " - 0s - loss: 0.0965 - acc: 0.9885\n",
      "Epoch 286/400\n",
      " - 0s - loss: 0.0968 - acc: 0.9885\n",
      "Epoch 287/400\n",
      " - 0s - loss: 0.0964 - acc: 0.9885\n",
      "Epoch 288/400\n",
      " - 0s - loss: 0.0955 - acc: 0.9885\n",
      "Epoch 289/400\n",
      " - 0s - loss: 0.0961 - acc: 0.9885\n",
      "Epoch 290/400\n",
      " - 0s - loss: 0.0952 - acc: 0.9885\n",
      "Epoch 291/400\n",
      " - 0s - loss: 0.0945 - acc: 0.9885\n",
      "Epoch 292/400\n",
      " - 0s - loss: 0.0938 - acc: 0.9885\n",
      "Epoch 293/400\n",
      " - 0s - loss: 0.0930 - acc: 0.9885\n",
      "Epoch 294/400\n",
      " - 0s - loss: 0.0923 - acc: 0.9885\n",
      "Epoch 295/400\n",
      " - 0s - loss: 0.0922 - acc: 0.9885\n",
      "Epoch 296/400\n",
      " - 0s - loss: 0.0918 - acc: 0.9885\n",
      "Epoch 297/400\n",
      " - 0s - loss: 0.0913 - acc: 0.9885\n",
      "Epoch 298/400\n",
      " - 0s - loss: 0.0910 - acc: 0.9885\n",
      "Epoch 299/400\n",
      " - 0s - loss: 0.0910 - acc: 0.9885\n",
      "Epoch 300/400\n",
      " - 0s - loss: 0.0922 - acc: 0.9885\n",
      "Epoch 301/400\n",
      " - 0s - loss: 0.0908 - acc: 1.0000\n",
      "Epoch 302/400\n",
      " - 0s - loss: 0.0908 - acc: 0.9885\n",
      "Epoch 303/400\n",
      " - 0s - loss: 0.0893 - acc: 0.9885\n",
      "Epoch 304/400\n",
      " - 0s - loss: 0.0889 - acc: 0.9885\n",
      "Epoch 305/400\n",
      " - 0s - loss: 0.0888 - acc: 0.9885\n",
      "Epoch 306/400\n",
      " - 0s - loss: 0.0881 - acc: 0.9885\n",
      "Epoch 307/400\n",
      " - 0s - loss: 0.0874 - acc: 0.9885\n",
      "Epoch 308/400\n",
      " - 0s - loss: 0.0872 - acc: 1.0000\n",
      "Epoch 309/400\n",
      " - 0s - loss: 0.0865 - acc: 0.9885\n",
      "Epoch 310/400\n",
      " - 0s - loss: 0.0855 - acc: 0.9885\n",
      "Epoch 311/400\n",
      " - 0s - loss: 0.0852 - acc: 1.0000\n",
      "Epoch 312/400\n",
      " - 0s - loss: 0.0850 - acc: 0.9885\n",
      "Epoch 313/400\n",
      " - 0s - loss: 0.0845 - acc: 0.9885\n",
      "Epoch 314/400\n",
      " - 0s - loss: 0.0851 - acc: 0.9885\n",
      "Epoch 315/400\n",
      " - 0s - loss: 0.0843 - acc: 1.0000\n",
      "Epoch 316/400\n",
      " - 0s - loss: 0.0840 - acc: 1.0000\n",
      "Epoch 317/400\n",
      " - 0s - loss: 0.0830 - acc: 1.0000\n",
      "Epoch 318/400\n",
      " - 0s - loss: 0.0829 - acc: 1.0000\n",
      "Epoch 319/400\n",
      " - 0s - loss: 0.0828 - acc: 1.0000\n",
      "Epoch 320/400\n",
      " - 0s - loss: 0.0823 - acc: 1.0000\n",
      "Epoch 321/400\n",
      " - 0s - loss: 0.0815 - acc: 1.0000\n",
      "Epoch 322/400\n",
      " - 0s - loss: 0.0817 - acc: 1.0000\n",
      "Epoch 323/400\n",
      " - 0s - loss: 0.0815 - acc: 1.0000\n",
      "Epoch 324/400\n",
      " - 0s - loss: 0.0812 - acc: 1.0000\n",
      "Epoch 325/400\n",
      " - 0s - loss: 0.0797 - acc: 1.0000\n",
      "Epoch 326/400\n",
      " - 0s - loss: 0.0802 - acc: 1.0000\n",
      "Epoch 327/400\n",
      " - 0s - loss: 0.0801 - acc: 1.0000\n",
      "Epoch 328/400\n",
      " - 0s - loss: 0.0788 - acc: 1.0000\n",
      "Epoch 329/400\n",
      " - 0s - loss: 0.0785 - acc: 1.0000\n",
      "Epoch 330/400\n",
      " - 0s - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 331/400\n",
      " - 0s - loss: 0.0782 - acc: 1.0000\n",
      "Epoch 332/400\n",
      " - 0s - loss: 0.0778 - acc: 1.0000\n",
      "Epoch 333/400\n",
      " - 0s - loss: 0.0773 - acc: 1.0000\n",
      "Epoch 334/400\n",
      " - 0s - loss: 0.0780 - acc: 1.0000\n",
      "Epoch 335/400\n",
      " - 0s - loss: 0.0771 - acc: 1.0000\n",
      "Epoch 336/400\n",
      " - 0s - loss: 0.0772 - acc: 1.0000\n",
      "Epoch 337/400\n",
      " - 0s - loss: 0.0765 - acc: 1.0000\n",
      "Epoch 338/400\n",
      " - 0s - loss: 0.0760 - acc: 1.0000\n",
      "Epoch 339/400\n",
      " - 0s - loss: 0.0761 - acc: 1.0000\n",
      "Epoch 340/400\n",
      " - 0s - loss: 0.0769 - acc: 1.0000\n",
      "Epoch 341/400\n",
      " - 0s - loss: 0.0767 - acc: 1.0000\n",
      "Epoch 342/400\n",
      " - 0s - loss: 0.0755 - acc: 1.0000\n",
      "Epoch 343/400\n",
      " - 0s - loss: 0.0749 - acc: 1.0000\n",
      "Epoch 344/400\n",
      " - 0s - loss: 0.0744 - acc: 1.0000\n",
      "Epoch 345/400\n",
      " - 0s - loss: 0.0743 - acc: 1.0000\n",
      "Epoch 346/400\n",
      " - 0s - loss: 0.0737 - acc: 1.0000\n",
      "Epoch 347/400\n",
      " - 0s - loss: 0.0731 - acc: 1.0000\n",
      "Epoch 348/400\n",
      " - 0s - loss: 0.0725 - acc: 1.0000\n",
      "Epoch 349/400\n",
      " - 0s - loss: 0.0725 - acc: 1.0000\n",
      "Epoch 350/400\n",
      " - 0s - loss: 0.0723 - acc: 1.0000\n",
      "Epoch 351/400\n",
      " - 0s - loss: 0.0720 - acc: 1.0000\n",
      "Epoch 352/400\n",
      " - 0s - loss: 0.0715 - acc: 1.0000\n",
      "Epoch 353/400\n",
      " - 0s - loss: 0.0715 - acc: 1.0000\n",
      "Epoch 354/400\n",
      " - 0s - loss: 0.0712 - acc: 1.0000\n",
      "Epoch 355/400\n",
      " - 0s - loss: 0.0707 - acc: 1.0000\n",
      "Epoch 356/400\n",
      " - 0s - loss: 0.0706 - acc: 1.0000\n",
      "Epoch 357/400\n",
      " - 0s - loss: 0.0704 - acc: 1.0000\n",
      "Epoch 358/400\n",
      " - 0s - loss: 0.0699 - acc: 1.0000\n",
      "Epoch 359/400\n",
      " - 0s - loss: 0.0698 - acc: 1.0000\n",
      "Epoch 360/400\n",
      " - 0s - loss: 0.0689 - acc: 1.0000\n",
      "Epoch 361/400\n",
      " - 0s - loss: 0.0701 - acc: 1.0000\n",
      "Epoch 362/400\n",
      " - 0s - loss: 0.0697 - acc: 1.0000\n",
      "Epoch 363/400\n",
      " - 0s - loss: 0.0688 - acc: 1.0000\n",
      "Epoch 364/400\n",
      " - 0s - loss: 0.0697 - acc: 1.0000\n",
      "Epoch 365/400\n",
      " - 0s - loss: 0.0685 - acc: 1.0000\n",
      "Epoch 366/400\n",
      " - 0s - loss: 0.0676 - acc: 1.0000\n",
      "Epoch 367/400\n",
      " - 0s - loss: 0.0670 - acc: 1.0000\n",
      "Epoch 368/400\n",
      " - 0s - loss: 0.0666 - acc: 1.0000\n",
      "Epoch 369/400\n",
      " - 0s - loss: 0.0663 - acc: 1.0000\n",
      "Epoch 370/400\n",
      " - 0s - loss: 0.0661 - acc: 1.0000\n",
      "Epoch 371/400\n",
      " - 0s - loss: 0.0661 - acc: 1.0000\n",
      "Epoch 372/400\n",
      " - 0s - loss: 0.0653 - acc: 1.0000\n",
      "Epoch 373/400\n",
      " - 0s - loss: 0.0652 - acc: 1.0000\n",
      "Epoch 374/400\n",
      " - 0s - loss: 0.0647 - acc: 1.0000\n",
      "Epoch 375/400\n",
      " - 0s - loss: 0.0642 - acc: 1.0000\n",
      "Epoch 376/400\n",
      " - 0s - loss: 0.0644 - acc: 1.0000\n",
      "Epoch 377/400\n",
      " - 0s - loss: 0.0640 - acc: 1.0000\n",
      "Epoch 378/400\n",
      " - 0s - loss: 0.0633 - acc: 1.0000\n",
      "Epoch 379/400\n",
      " - 0s - loss: 0.0661 - acc: 1.0000\n",
      "Epoch 380/400\n",
      " - 0s - loss: 0.0651 - acc: 1.0000\n",
      "Epoch 381/400\n",
      " - 0s - loss: 0.0653 - acc: 1.0000\n",
      "Epoch 382/400\n",
      " - 0s - loss: 0.0634 - acc: 1.0000\n",
      "Epoch 383/400\n",
      " - 0s - loss: 0.0632 - acc: 1.0000\n",
      "Epoch 384/400\n",
      " - 0s - loss: 0.0626 - acc: 1.0000\n",
      "Epoch 385/400\n",
      " - 0s - loss: 0.0621 - acc: 1.0000\n",
      "Epoch 386/400\n",
      " - 0s - loss: 0.0618 - acc: 1.0000\n",
      "Epoch 387/400\n",
      " - 0s - loss: 0.0614 - acc: 1.0000\n",
      "Epoch 388/400\n",
      " - 0s - loss: 0.0612 - acc: 1.0000\n",
      "Epoch 389/400\n",
      " - 0s - loss: 0.0610 - acc: 1.0000\n",
      "Epoch 390/400\n",
      " - 0s - loss: 0.0609 - acc: 1.0000\n",
      "Epoch 391/400\n",
      " - 0s - loss: 0.0603 - acc: 1.0000\n",
      "Epoch 392/400\n",
      " - 0s - loss: 0.0602 - acc: 1.0000\n",
      "Epoch 393/400\n",
      " - 0s - loss: 0.0596 - acc: 1.0000\n",
      "Epoch 394/400\n",
      " - 0s - loss: 0.0599 - acc: 1.0000\n",
      "Epoch 395/400\n",
      " - 0s - loss: 0.0593 - acc: 1.0000\n",
      "Epoch 396/400\n",
      " - 0s - loss: 0.0591 - acc: 1.0000\n",
      "Epoch 397/400\n",
      " - 0s - loss: 0.0588 - acc: 1.0000\n",
      "Epoch 398/400\n",
      " - 0s - loss: 0.0585 - acc: 1.0000\n",
      "Epoch 399/400\n",
      " - 0s - loss: 0.0582 - acc: 1.0000\n",
      "Epoch 400/400\n",
      " - 0s - loss: 0.0580 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2870d03d470>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_X_train, y_train, epochs= 400, verbose= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spits out probabilities by default.\n",
    "# model.predict(scaled_X_test)\n",
    "\n",
    "# model.predict_classes(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(scaled_X_test)\n",
    "\n",
    "# we have the answers because we have the y_test vector\n",
    "conf_mat = confusion_matrix(y_test, predictions)\n",
    "# [[True Negative, False Negative],\n",
    "# [False Positive, True Positive]]\n",
    "\n",
    "confusion = {\n",
    "    \"TruePositive\": conf_mat[1][1],\n",
    "    \"TrueNegative\": conf_mat[0][0],\n",
    "    \"FalsePositive\": conf_mat[1][0],\n",
    "    \"FalseNegative\": conf_mat[0][1],\n",
    "}\n",
    "# confusion[\"TrueNegative\"]\n",
    "accuracy = (confusion[\"TrueNegative\"] + confusion[\"TruePositive\"]) / sum(confusion.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "\t---------------------------------\n",
      "\t|\t21\t|\t11\t| \n",
      "\t---------------------------------\n",
      "\t|\t7\t|\t5\t|\n",
      "\t---------------------------------\n",
      "\n",
      "TruePositive\t:\t21\n",
      "TrueNegative\t:\t5\n",
      "FalsePositive\t:\t11\n",
      "FalseNegative\t:\t7\n"
     ]
    }
   ],
   "source": [
    "def printDict(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            if hasattr(v, '__iter__'):\n",
    "                print(k)\n",
    "                printDict(v)\n",
    "            else:\n",
    "                print('%s\\t:\\t%s' % (k, v))\n",
    "    elif isinstance(obj, list):\n",
    "        for v in obj:\n",
    "            if hasattr(v, '__iter__'):\n",
    "                printDict(v)\n",
    "            else:\n",
    "                print(v)\n",
    "    else:\n",
    "        print(obj)\n",
    "\n",
    "separator = \"\\t---------------------------------\\n\"\n",
    "print(f'\\nConfusion Matrix:\\n{separator}\\t|\\t{ confusion[\"TruePositive\"] }\\t|\\t{confusion[\"FalsePositive\"]}\\t| \\n{separator}\\t|\\t{confusion[\"FalseNegative\"]}\\t|\\t{confusion[\"TrueNegative\"]}\\t|\\n{separator}')\n",
    "printDict(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\taccuracy = 59.09%\n",
      "\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "   Metastasis, class 0       0.31      0.42      0.36        12\n",
      "No metastasis, class 1       0.75      0.66      0.70        32\n",
      "\n",
      "           avg / total       0.63      0.59      0.61        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the metrics\n",
    "print(f\"\\n\\tACCURACY = {round(accuracy * 100, 2)}%\\n\")\n",
    "print(classification_report(y_test, predictions, target_names=TARGET_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('./models/denseModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "# from keras.models import load_model\n",
    "# newModel = load_model('./models/denseModel.h5')\n",
    "\n",
    "# use the loaded model to predict classes\n",
    "# x_test is already after scaling!\n",
    "# newModel.predict_classes(scaled_X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-cvcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
